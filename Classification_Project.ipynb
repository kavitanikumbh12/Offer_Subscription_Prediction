{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model for Binary Classification\n",
    "\n",
    "A retail banking institution is going to float a stock trading facility for their existing customer. Since this kind of facitlity is nothing new , company knows that they will have to incetivise their customers for adopting their offerings. One way to incetiwise is to offer discounts on the commision for trading transactions.\n",
    "\n",
    "One issue with that is that only about 10% of the customers do enought trades for earnings after discounts to be profitable. Company wants to figure out, which are those 10% customer so that it can selectively offer them discount. there is no magic way to figure that out. So company rolled out this service to about 10000+ of their customers and observed their trading behaviour for 6 months and after that they labelled them into two revenue.grids 1 and 2.\n",
    "using this data, now they want us to build a classification model which can be used to classify their remaining customers into these revenue grids.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression from Scikit Learn \n",
    "\n",
    "Logistic Regression in scikit learn already contains penalties. l1 and l2 [Read as L-one & L-two] penalties . l1 penalty is same as lasso penalty where as l2 is same as ridge penalty. parameter C for logistic regression function is the hyperparameter for penalty . However it works in inverse fashion, i.e. if C takes smalle , it means higher penalty. \n",
    "\n",
    "For the case that we have discussed here , we have discussed l1 penalty with value of C as 1. We have left following things for you to try on your own.\n",
    "\n",
    "* model with l2 penalty\n",
    "* Finding optimal value of hyperparameter C with cross-validation for both the penalties\n",
    "\n",
    "You will find these in the practice exercise as well\n",
    "\n",
    "you can use auc value obtained from function roc_auc_score to select best value for the hyperparameter. Higher the auc, better is the model . If you dont recall this, please go back to the theoretical reading material.\n",
    "\n",
    "Lets beging our model building process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\s348a\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\s348a\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\s348a\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\s348a\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\s348a\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\s348a\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\s348a\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\s348a\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=r'Existing Base.csv'\n",
    "bd=pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REF_NO</th>\n",
       "      <th>children</th>\n",
       "      <th>age_band</th>\n",
       "      <th>status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>occupation_partner</th>\n",
       "      <th>home_status</th>\n",
       "      <th>family_income</th>\n",
       "      <th>self_employed</th>\n",
       "      <th>self_employed_partner</th>\n",
       "      <th>...</th>\n",
       "      <th>Investment Tax Saving Bond</th>\n",
       "      <th>Home Loan</th>\n",
       "      <th>Online Purchase Amount</th>\n",
       "      <th>Revenue Grid</th>\n",
       "      <th>gender</th>\n",
       "      <th>region</th>\n",
       "      <th>Investment in Commudity</th>\n",
       "      <th>Investment in Equity</th>\n",
       "      <th>Investment in Derivative</th>\n",
       "      <th>Portfolio Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Zero</td>\n",
       "      <td>51-55</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Manual Worker</td>\n",
       "      <td>Secretarial/Admin</td>\n",
       "      <td>Own Home</td>\n",
       "      <td>&lt;17,500, &gt;=15,000</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>Wales</td>\n",
       "      <td>74.67</td>\n",
       "      <td>18.66</td>\n",
       "      <td>32.32</td>\n",
       "      <td>89.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Zero</td>\n",
       "      <td>55-60</td>\n",
       "      <td>Single/Never Married</td>\n",
       "      <td>Retired</td>\n",
       "      <td>Retired</td>\n",
       "      <td>Own Home</td>\n",
       "      <td>&lt;27,500, &gt;=25,000</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>North West</td>\n",
       "      <td>20.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>22.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Zero</td>\n",
       "      <td>26-30</td>\n",
       "      <td>Single/Never Married</td>\n",
       "      <td>Professional</td>\n",
       "      <td>Other</td>\n",
       "      <td>Own Home</td>\n",
       "      <td>&lt;30,000, &gt;=27,500</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>North</td>\n",
       "      <td>98.06</td>\n",
       "      <td>31.07</td>\n",
       "      <td>80.96</td>\n",
       "      <td>171.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Zero</td>\n",
       "      <td>18-21</td>\n",
       "      <td>Single/Never Married</td>\n",
       "      <td>Professional</td>\n",
       "      <td>Manual Worker</td>\n",
       "      <td>Own Home</td>\n",
       "      <td>&lt;15,000, &gt;=12,500</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>West Midlands</td>\n",
       "      <td>4.10</td>\n",
       "      <td>14.15</td>\n",
       "      <td>17.57</td>\n",
       "      <td>-41.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Zero</td>\n",
       "      <td>45-50</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Business Manager</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Own Home</td>\n",
       "      <td>&lt;30,000, &gt;=27,500</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.91</td>\n",
       "      <td>25.98</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>70.16</td>\n",
       "      <td>55.86</td>\n",
       "      <td>80.44</td>\n",
       "      <td>235.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   REF_NO children age_band                status        occupation  \\\n",
       "0       1     Zero    51-55               Partner     Manual Worker   \n",
       "1       2     Zero    55-60  Single/Never Married           Retired   \n",
       "2       3     Zero    26-30  Single/Never Married      Professional   \n",
       "3       5     Zero    18-21  Single/Never Married      Professional   \n",
       "4       6     Zero    45-50               Partner  Business Manager   \n",
       "\n",
       "  occupation_partner home_status      family_income self_employed  \\\n",
       "0  Secretarial/Admin    Own Home  <17,500, >=15,000            No   \n",
       "1            Retired    Own Home  <27,500, >=25,000            No   \n",
       "2              Other    Own Home  <30,000, >=27,500           Yes   \n",
       "3      Manual Worker    Own Home  <15,000, >=12,500            No   \n",
       "4            Unknown    Own Home  <30,000, >=27,500            No   \n",
       "\n",
       "  self_employed_partner  ...  Investment Tax Saving Bond Home Loan  \\\n",
       "0                    No  ...                       19.99      0.00   \n",
       "1                    No  ...                        0.00      0.00   \n",
       "2                    No  ...                        0.00      3.49   \n",
       "3                    No  ...                        0.00      0.00   \n",
       "4                    No  ...                        0.00     45.91   \n",
       "\n",
       "  Online Purchase Amount Revenue Grid  gender         region  \\\n",
       "0                   0.00            1  Female          Wales   \n",
       "1                   0.00            2  Female     North West   \n",
       "2                   0.00            2    Male          North   \n",
       "3                   0.00            2  Female  West Midlands   \n",
       "4                  25.98            2  Female       Scotland   \n",
       "\n",
       "   Investment in Commudity  Investment in Equity  Investment in Derivative  \\\n",
       "0                    74.67                 18.66                     32.32   \n",
       "1                    20.19                  0.00                      4.33   \n",
       "2                    98.06                 31.07                     80.96   \n",
       "3                     4.10                 14.15                     17.57   \n",
       "4                    70.16                 55.86                     80.44   \n",
       "\n",
       "   Portfolio Balance  \n",
       "0              89.43  \n",
       "1              22.78  \n",
       "2             171.78  \n",
       "3             -41.70  \n",
       "4             235.02  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Zero    6208\n",
       "1       1848\n",
       "2       1607\n",
       "3        473\n",
       "4+        19\n",
       "Name: children, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd[\"children\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we can directly convert this to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.loc[bd[\"children\"]==\"Zero\",\"children\"]=\"0\"\n",
    "bd.loc[bd[\"children\"]==\"4+\",\"children\"]=\"4\"\n",
    "bd[\"children\"]=pd.to_numeric(bd[\"children\"],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    9069\n",
       "1    1086\n",
       "Name: Revenue Grid, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd[\"Revenue Grid\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd[\"y\"]=np.where(bd[\"Revenue Grid\"]==2,0,1)\n",
    "bd=bd.drop([\"Revenue Grid\"],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variable , age_band if we treat it as categorical variable , we can combine its categories by looking average response rate across its categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_band\n",
       "18-21      0.17\n",
       "22-25      0.11\n",
       "26-30      0.11\n",
       "31-35      0.11\n",
       "36-40      0.13\n",
       "41-45      0.11\n",
       "45-50      0.10\n",
       "51-55      0.10\n",
       "55-60      0.11\n",
       "61-65      0.09\n",
       "65-70      0.10\n",
       "71+        0.10\n",
       "Unknown    0.05\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(bd.groupby(\"age_band\")[\"y\"].mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unknown</th>\n",
       "      <th>ab_10</th>\n",
       "      <th>ab_11</th>\n",
       "      <th>ab_13</th>\n",
       "      <th>ab_17</th>\n",
       "      <th>ab_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unknown  ab_10  ab_11  ab_13  ab_17  ab_9\n",
       "0        0      1      0      0      0     0\n",
       "1        0      0      1      0      0     0\n",
       "2        0      0      1      0      0     0\n",
       "3        0      0      0      0      1     0\n",
       "4        0      1      0      0      0     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(bd)):\n",
    "    if bd[\"age_band\"][i] in [\"71+\",\"65-70\",\"51-55\",\"45-50\"]:\n",
    "        bd.loc[i,\"age_band\"]=\"ab_10\"\n",
    "    if bd[\"age_band\"][i] in [\"55-60\",\"41-45\",\"31-35\",\"22-25\",\"26-30\"]:\n",
    "        bd.loc[i,\"age_band\"]=\"ab_11\"\n",
    "    if bd[\"age_band\"][i]==\"36-40\":\n",
    "        bd.loc[i,\"age_band\"]=\"ab_13\"\n",
    "    if bd[\"age_band\"][i]==\"18-21\":\n",
    "        bd.loc[i,\"age_band\"]=\"ab_17\"\n",
    "    if bd[\"age_band\"][i]==\"61-65\":\n",
    "        bd.loc[i,\"age_band\"]=\"ab_9\"\n",
    "ab_dummies=pd.get_dummies(bd[\"age_band\"])\n",
    "ab_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add it back to the dataset, dropping the dummy for \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd=pd.concat([bd,ab_dummies],1)\n",
    "bd=bd.drop([\"age_band\",\"Unknown\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Partner                 7709\n",
       "Single/Never Married    1101\n",
       "Divorced/Separated       679\n",
       "Widowed                  618\n",
       "Unknown                   48\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd[\"status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd[\"st_partner\"]=np.where(bd[\"status\"]==\"Partner\",1,0)\n",
    "bd[\"st_singleNm\"]=np.where(bd[\"status\"]==\"Single/Never Married\",1,0)\n",
    "bd[\"st_divSep\"]=np.where(bd[\"status\"]==\"Divorced/Separated\",1,0)\n",
    "bd=bd.drop([\"status\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "occupation\n",
       "Business Manager     0.12\n",
       "Housewife            0.09\n",
       "Manual Worker        0.11\n",
       "Other                0.11\n",
       "Professional         0.12\n",
       "Retired              0.10\n",
       "Secretarial/Admin    0.11\n",
       "Student              0.11\n",
       "Unknown              0.11\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(bd.groupby(\"occupation\")[\"y\"].mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Housewife</th>\n",
       "      <th>oc_10</th>\n",
       "      <th>oc_11</th>\n",
       "      <th>oc_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Housewife  oc_10  oc_11  oc_12\n",
       "0          0      0      1      0\n",
       "1          0      1      0      0\n",
       "2          0      0      0      1\n",
       "3          0      0      0      1\n",
       "4          0      0      0      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(bd)):\n",
    "    if bd[\"occupation\"][i] in [\"Unknown\",\"Student\",\"Secretarial/Admin\",\"Other\",\"Manual Worker\"]:\n",
    "        bd.loc[i,\"occupation\"]=\"oc_11\"\n",
    "    if bd[\"occupation\"][i] in [\"Professional\",\"Business Manager\"]:\n",
    "        bd.loc[i,\"occupation\"]=\"oc_12\"\n",
    "    if bd[\"occupation\"][i]==\"Retired\":\n",
    "        bd.loc[i,\"occupation\"]=\"oc_10\"\n",
    "oc_dummies=pd.get_dummies(bd[\"occupation\"])\n",
    "oc_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd=pd.concat([bd,oc_dummies],1)\n",
    "\n",
    "bd=bd.drop([\"occupation\",\"Housewife\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "occupation_partner\n",
       "Business Manager     0.11\n",
       "Housewife            0.11\n",
       "Manual Worker        0.11\n",
       "Other                0.10\n",
       "Professional         0.11\n",
       "Retired              0.10\n",
       "Secretarial/Admin    0.12\n",
       "Student              0.12\n",
       "Unknown              0.10\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(bd.groupby(\"occupation_partner\")[\"y\"].mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd[\"ocp_10\"]=0\n",
    "bd[\"ocp_12\"]=0\n",
    "for i in range(len(bd)):\n",
    "    if bd[\"occupation_partner\"][i] in [\"Unknown\",\"Retired\",\"Other\"]:\n",
    "        bd.loc[i,\"ocp_10\"]=1\n",
    "    if bd[\"occupation_partner\"][i] in [\"Student\",\"Secretarial/Admin\"]:\n",
    "        bd.loc[i,\"ocp_12\"]=1\n",
    "        \n",
    "bd=bd.drop([\"occupation_partner\",\"TVarea\",\"post_code\",\"post_area\",\"region\"],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have also dropped variables TVarea, region, post_code, post_area. If you look at number of unique values taken by post_area and post_code , you'll realise why decided to drop them. TVarea and region on the other hand we have left for you to make use of and see if using them improves your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Own Home                9413\n",
       "Rent from Council/HA     322\n",
       "Rent Privately           261\n",
       "Live in Parental Hom     109\n",
       "Unclassified              50\n",
       "Name: home_status, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd[\"home_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd[\"hs_own\"]=np.where(bd[\"home_status\"]==\"Own Home\",1,0)\n",
    "del bd[\"home_status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used an alternate syntax for dropping a column here. You can use that too if you like this syntax better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Female     7634\n",
       "Male       2486\n",
       "Unknown      35\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd[\"gender_f\"]=np.where(bd[\"gender\"]==\"Female\",1,0)\n",
    "del bd[\"gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     9436\n",
       "Yes     719\n",
       "Name: self_employed, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd[\"self_employed\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd[\"semp_yes\"]=np.where(bd[\"self_employed\"]==\"Yes\",1,0)\n",
    "del bd[\"self_employed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     9026\n",
       "Yes    1129\n",
       "Name: self_employed_partner, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd[\"self_employed_partner\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd[\"semp_part_yes\"]=np.where(bd[\"self_employed_partner\"]==\"Yes\",1,0)\n",
    "del bd[\"self_employed_partner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ">=35,000             2517\n",
       "<27,500, >=25,000    1227\n",
       "<30,000, >=27,500     994\n",
       "<25,000, >=22,500     833\n",
       "<20,000, >=17,500     683\n",
       "<12,500, >=10,000     677\n",
       "<17,500, >=15,000     634\n",
       "<15,000, >=12,500     629\n",
       "<22,500, >=20,000     590\n",
       "<10,000, >= 8,000     563\n",
       "< 8,000, >= 4,000     402\n",
       "< 4,000               278\n",
       "Unknown               128\n",
       "Name: family_income, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd[\"family_income\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert this to number as average of the range once we have figured out what to do with category \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "family_income\n",
       "< 4,000              0.0755\n",
       "< 8,000, >= 4,000    0.0796\n",
       "<10,000, >= 8,000    0.1066\n",
       "<12,500, >=10,000    0.1019\n",
       "<15,000, >=12,500    0.1113\n",
       "<17,500, >=15,000    0.1230\n",
       "<20,000, >=17,500    0.1113\n",
       "<22,500, >=20,000    0.1186\n",
       "<25,000, >=22,500    0.1032\n",
       "<27,500, >=25,000    0.0970\n",
       "<30,000, >=27,500    0.1157\n",
       ">=35,000             0.1116\n",
       "Unknown              0.0703\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(bd.groupby(\"family_income\")[\"y\"].mean(),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd[\"fi\"]=4 # by doing this , we have essentially clubbed <4000 and Unknown values . How?\n",
    "bd.loc[bd[\"family_income\"]==\"< 8,000, >= 4,000\",\"fi\"]=6\n",
    "bd.loc[bd[\"family_income\"]==\"<10,000, >= 8,000\",\"fi\"]=9\n",
    "bd.loc[bd[\"family_income\"]==\"<12,500, >=10,000\",\"fi\"]=11.25\n",
    "bd.loc[bd[\"family_income\"]==\"<15,000, >=12,500\",\"fi\"]=13.75\n",
    "bd.loc[bd[\"family_income\"]==\"<17,500, >=15,000\",\"fi\"]=16.25\n",
    "bd.loc[bd[\"family_income\"]==\"<20,000, >=17,500\",\"fi\"]=18.75\n",
    "bd.loc[bd[\"family_income\"]==\"<22,500, >=20,000\",\"fi\"]=21.25\n",
    "bd.loc[bd[\"family_income\"]==\"<25,000, >=22,500\",\"fi\"]=23.75\n",
    "bd.loc[bd[\"family_income\"]==\"<27,500, >=25,000\",\"fi\"]=26.25\n",
    "bd.loc[bd[\"family_income\"]==\"<30,000, >=27,500\",\"fi\"]=28.75\n",
    "bd.loc[bd[\"family_income\"]==\">=35,000\",\"fi\"]=35\n",
    "bd=bd.drop([\"family_income\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REF_NO                               int64\n",
       "children                             int64\n",
       "year_last_moved                      int64\n",
       "Average Credit Card Transaction    float64\n",
       "Balance Transfer                   float64\n",
       "Term Deposit                       float64\n",
       "Life Insurance                     float64\n",
       "Medical Insurance                  float64\n",
       "Average A/C Balance                float64\n",
       "Personal Loan                      float64\n",
       "Investment in Mutual Fund          float64\n",
       "Investment Tax Saving Bond         float64\n",
       "Home Loan                          float64\n",
       "Online Purchase Amount             float64\n",
       "Investment in Commudity            float64\n",
       "Investment in Equity               float64\n",
       "Investment in Derivative           float64\n",
       "Portfolio Balance                  float64\n",
       "y                                    int32\n",
       "ab_10                                uint8\n",
       "ab_11                                uint8\n",
       "ab_13                                uint8\n",
       "ab_17                                uint8\n",
       "ab_9                                 uint8\n",
       "st_partner                           int32\n",
       "st_singleNm                          int32\n",
       "st_divSep                            int32\n",
       "oc_10                                uint8\n",
       "oc_11                                uint8\n",
       "oc_12                                uint8\n",
       "ocp_10                               int64\n",
       "ocp_12                               int64\n",
       "hs_own                               int32\n",
       "gender_f                             int32\n",
       "semp_yes                             int32\n",
       "semp_part_yes                        int32\n",
       "fi                                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the entire data is of numeric type, lets beging our modelling process after removing nas from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.dropna(axis=0,inplace=True)\n",
    "bd_train, bd_test = train_test_split(bd, test_size = 0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=bd_train.drop([\"y\",\"REF_NO\"],1)\n",
    "y_train=bd_train[\"y\"]\n",
    "x_test=bd_test.drop([\"y\",\"REF_NO\"],1)\n",
    "y_test=bd_test[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      " |      liblinear solver), no regularization is applied.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      " |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      " |      - 'saga' also supports 'elasticnet' penalty\n",
      " |      - 'liblinear' does not support setting ``penalty='none'``\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr=LogisticRegression(penalty=\"l2\",class_weight=\"balanced\",random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s348a\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', random_state=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8912537552375683"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score model performance on the test data\n",
    "roc_auc_score(y_test,logr.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To arrive at the eventual 1,0 prediction , we need to find some way [some cutoff ] to convert predicted probabilites into two classes. Lets first get the probabilities out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_score=pd.Series(list(zip(*logr.predict_proba(x_train)))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On these scores , we will consider many cutoffs between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs=np.linspace(0,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these cutoff , we are going to look at TP,FP,TN,FN values and caluclate KS. Then we'll chose the best cutoff as the one having highest KS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "KS_cut=[]\n",
    "for cutoff in cutoffs:\n",
    "    predicted=pd.Series([0]*len(y_train))\n",
    "    predicted[prob_score>cutoff]=1\n",
    "    df=pd.DataFrame(list(zip(y_train,predicted)),columns=[\"real\",\"predicted\"])\n",
    "    TP=len(df[(df[\"real\"]==1) &(df[\"predicted\"]==1) ])\n",
    "    FP=len(df[(df[\"real\"]==0) &(df[\"predicted\"]==1) ])\n",
    "    TN=len(df[(df[\"real\"]==0) &(df[\"predicted\"]==0) ])\n",
    "    FN=len(df[(df[\"real\"]==1) &(df[\"predicted\"]==0) ])\n",
    "    P=TP+FN\n",
    "    N=TN+FP\n",
    "    KS=(TP/P)-(FP/N)\n",
    "    KS_cut.append(KS)\n",
    "\n",
    "cutoff_data=pd.DataFrame(list(zip(cutoffs,KS_cut)),columns=[\"cutoff\",\"KS\"])\n",
    "\n",
    "KS_cutoff=cutoff_data[cutoff_data[\"KS\"]==cutoff_data[\"KS\"].max()][\"cutoff\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll see how this model with the cutoff determined here , performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix :\n",
      " \n",
      "  predicted     0    1\n",
      "real                \n",
      "0          1635  172\n",
      "1            26  198\n"
     ]
    }
   ],
   "source": [
    "# Performance on test data\n",
    "prob_score_test=pd.Series(list(zip(*logr.predict_proba(x_test)))[1])\n",
    "\n",
    "predicted_test=pd.Series([0]*len(y_test))\n",
    "predicted_test[prob_score_test>float(KS_cutoff)]=1\n",
    "\n",
    "df_test=pd.DataFrame(list(zip(y_test,predicted_test)),columns=[\"real\",\"predicted\"])\n",
    "\n",
    "k=pd.crosstab(df_test['real'],df_test[\"predicted\"])\n",
    "print('confusion matrix :\\n \\n ',k)\n",
    "TN=k.iloc[0,0]\n",
    "TP=k.iloc[1,1]\n",
    "FP=k.iloc[0,1]\n",
    "FN=k.iloc[1,0]\n",
    "P=TP+FN\n",
    "N=TN+FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9025110782865583"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of test\n",
    "(TP+TN)/(P+N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8839285714285714"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sensitivity on test\n",
    "TP/P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904814609850581"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specificity on test\n",
    "TN/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we see how cutoff determined by F_beta score performs on test data for beta values : 0.5,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs=np.linspace(0.010,0.99,100)\n",
    "def Fbeta_perf(beta,cutoffs,y_train,prob_score):\n",
    "    FB_cut=[]\n",
    "    for cutoff in cutoffs:\n",
    "        predicted=pd.Series([0]*len(y_train))\n",
    "        predicted[prob_score>cutoff]=1\n",
    "        df=pd.DataFrame(list(zip(y_train,predicted)),columns=[\"real\",\"predicted\"])\n",
    "\n",
    "        TP=len(df[(df[\"real\"]==1) &(df[\"predicted\"]==1) ])\n",
    "        FP=len(df[(df[\"real\"]==0) &(df[\"predicted\"]==1) ])\n",
    "        FN=len(df[(df[\"real\"]==1) &(df[\"predicted\"]==0) ])\n",
    "        P=TP+FN\n",
    "        \n",
    "        \n",
    "        Precision=TP/(TP+FP)\n",
    "        Recall=TP/P\n",
    "        FB=(1+beta**2)*Precision*Recall/((beta**2)*Precision+Recall)\n",
    "        FB_cut.append(FB)\n",
    "\n",
    "    cutoff_data=pd.DataFrame(list(zip(cutoffs,FB_cut)),columns=[\"cutoff\",\"FB\"])\n",
    "\n",
    "    FB_cutoff=cutoff_data[cutoff_data[\"FB\"]==cutoff_data[\"FB\"].max()][\"cutoff\"]\n",
    "\n",
    "    prob_score_test=pd.Series(list(zip(*logr.predict_proba(x_test)))[1])\n",
    "\n",
    "    predicted_test=pd.Series([0]*len(y_test))\n",
    "    predicted_test[prob_score_test>float(FB_cutoff)]=1\n",
    "\n",
    "    df_test=pd.DataFrame(list(zip(y_test,predicted_test)),columns=[\"real\",\"predicted\"])\n",
    "\n",
    "    k=pd.crosstab(df_test['real'],df_test[\"predicted\"])\n",
    "#     print('confusion matrix :\\n \\n ',k)\n",
    "    TN=k.iloc[0,0]\n",
    "    TP=k.iloc[1,1]\n",
    "    FP=k.iloc[0,1]\n",
    "    FN=k.iloc[1,0]\n",
    "    P=TP+FN\n",
    "    N=TN+FP\n",
    "    print('For beta :',beta)\n",
    "    print('Accuracy is :',(TP+TN)/(P+N))\n",
    "    print('Sensitivity is :',(TP/P))\n",
    "    print('Specificity is :',(TN/N))\n",
    "    print('\\n \\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For beta : 0.5\n",
      "Accuracy is : 0.9345150172328902\n",
      "Sensitivity is : 0.5044642857142857\n",
      "Specificity is : 0.987825124515772\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "For beta : 1\n",
      "Accuracy is : 0.9290989660265879\n",
      "Sensitivity is : 0.7366071428571429\n",
      "Specificity is : 0.9529607083563918\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "For beta : 2\n",
      "Accuracy is : 0.9162973904480551\n",
      "Sensitivity is : 0.8526785714285714\n",
      "Specificity is : 0.9241837299391257\n",
      "\n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Fbeta_perf(0.5,cutoffs,y_train,prob_score)\n",
    "Fbeta_perf(1,cutoffs,y_train,prob_score)\n",
    "Fbeta_perf(2,cutoffs,y_train,prob_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that low beta < 1 favors Specificity where as beta > 1 favors sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
